{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 2 for Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model takes some queues from several different sources, combined into one supermodel! Kind of. I took some cool ideas from Elena Cuoco, who wrote a blog post on this specific dataset. I primarily looked at some feature engineering tactics she used. On the algorithmic side, I implemented something similar to the 'improving you submisson' DataQuest tutorial, which used a combination of logistic regression and random forests to predict survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've started off by defining a data cleaning convenience method, so I won't have to duplicate code later when dealing with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    # Encoders to be used later to make converting categories to numerical representations easier\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    # Replace 'male' 'Sex' with 0, 'female' 'Sex' with 1\n",
    "    df.loc[df['Sex'] == 'male', 'Sex'] = 0\n",
    "    df.loc[df['Sex'] == 'female', 'Sex'] = 1\n",
    "    \n",
    "    # Setting likely wrong values to nan\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    \n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "            'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "            'Don', 'Jonkheer']\n",
    "    \n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "    \n",
    "    # Collapse all titles into four main categories - Mr, Mrs, Master, and Miss\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Mr'\n",
    "        elif title in ['Master']:\n",
    "            return 'Master'\n",
    "        elif title in ['Countess', 'Mme','Mrs']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms','Miss']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        elif title =='':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Master'\n",
    "            else:\n",
    "                return 'Miss'\n",
    "        else:\n",
    "            return title\n",
    "        \n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "    \n",
    "    #Creating new family_size column\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    df['Family']=df['SibSp']*df['Parch']\n",
    "\n",
    "    # Replace 'Embarked' NaN with most popular 'Embarked', 'S'\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    \n",
    "    # Replace 'Embarked' values with numeric values 0-2\n",
    "    df.loc[df['Embarked'] == 'S', 'Embarked'] = 0\n",
    "    df.loc[df['Embarked'] == 'C', 'Embarked'] = 1\n",
    "    df.loc[df['Embarked'] == 'Q', 'Embarked'] = 2\n",
    "    \n",
    "    # Imputing Nan values in fare based on median values for each class\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==1),'Fare'] =np.median(df[df['Pclass'] == 1]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==2),'Fare'] =np.median( df[df['Pclass'] == 2]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==3),'Fare'] = np.median(df[df['Pclass'] == 3]['Fare'].dropna())\n",
    "    \n",
    "    # Trying to improve upon the base 'Age' filling median\n",
    "    df['AgeFill']=df['Age']\n",
    "    mean_ages = np.zeros(4)\n",
    "    mean_ages[0]=np.average(df[df['Title'] == 'Miss']['Age'].dropna())\n",
    "    mean_ages[1]=np.average(df[df['Title'] == 'Mrs']['Age'].dropna())\n",
    "    mean_ages[2]=np.average(df[df['Title'] == 'Mr']['Age'].dropna())\n",
    "    mean_ages[3]=np.average(df[df['Title'] == 'Master']['Age'].dropna())\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Miss') ,'AgeFill'] = mean_ages[0]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mrs') ,'AgeFill'] = mean_ages[1]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mr') ,'AgeFill'] = mean_ages[2]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Master') ,'AgeFill'] = mean_ages[3]\n",
    "\n",
    "    # Creating a new category 'AgeCat'\n",
    "    df['AgeCat'] = df['AgeFill']\n",
    "    df.loc[ (df.AgeFill<=10) ,'AgeCat'] = 'child'\n",
    "    df.loc[ (df.AgeFill>60),'AgeCat'] = 'aged'\n",
    "    df.loc[ (df.AgeFill>10) & (df.AgeFill <=30) ,'AgeCat'] = 'adult'\n",
    "    df.loc[ (df.AgeFill>30) & (df.AgeFill <=60) ,'AgeCat'] = 'senior'\n",
    "    \n",
    "    label_encoder.fit(df['AgeCat'])\n",
    "    x_age = label_encoder.transform(df['AgeCat'])\n",
    "    df['AgeCat'] = x_age.astype(np.float)\n",
    "    \n",
    "    label_encoder.fit(df['Title'])\n",
    "    x_title=label_encoder.transform(df['Title'])\n",
    "    df['Title'] =x_title.astype(np.float)    \n",
    "    \n",
    "    df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "# Utility to clean data\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if string.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    print big_string\n",
    "    return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll use the above cleaning function to clean up the training data set and generate new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "titanic = clean_data(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll try tuning some parameters on the random forest model. I'm specifically looking at n_estimators, min_samples_split, and min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83164983165\n"
     ]
    }
   ],
   "source": [
    "predictors = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Family_Size', 'Family', 'AgeFill', 'AgeCat', 'Fare_Per_Person']\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little worrying - while Random Forests seems appropriate for a use like this where there may be a number of predictors with relatively unknown weight, I think I'm probably overfitting quite a bit to the training set. This makes me a little hesitant to continue, but I imagine I'll end up somewhere in the 79% range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll leverage some features of scikit learn to select the most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAFICAYAAACbYK+nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmUpGV99vHvxYwLoOBgYMYouwqoATEYSDSxFRNNokJE\nISgGUF/jOS5E36igJzKSuBE1EYz6ugQnKioEkSUqI0K7JWwCgggTESUu0B4QECQJAtf7x/3UTE1P\nr9PPU9V39fU5p85UPV1Vv7u6p3511++5F9kmIiLqsMWwGxAREXOXpB0RUZEk7YiIiiRpR0RUJEk7\nIqIiSdoRERWZNWlLeqykKyRd3vx7h6TXSlohaa2kdZLOk7TtIBocEbGUaT7jtCVtAfwE2B94NXCr\n7RMlvQlYYfvYbpoZEREw//LIM4Ef2P4xcBCwpjm+Bji4zYZFRMSm5pu0DwNOba6vtD0BYPtmYIc2\nGxYREZuac3lE0gOAnwF72b5F0i9sb9f381ttP3yKx2WefETEZrCtycfm09P+Y+Dbtm9pbk9IWgkg\naRXw8xkCD+xy/PHHJ16l8Ub5tSVe/fEGfZnOfJL24cBn+m6fDRzVXD8SOGsezxUREZthTklb0laU\nk5Cf7zv8buAPJa0DDgTe1X7zIiKi35yStu27bW9v+86+Y7+w/Uzbe9j+I9u3d9fMuTvppA8jqfXL\nqlW7TBlvbGxsoK9vlOON8mtLvPrjLRbzGqe9WQEkdx1jUjygi3iasc4UEdEmSXiBJyIjImLIkrQj\nIiqSpB0RUZEk7YiIiiRpR0RUJEk7IqIiSdoRERVJ0o6IqEiSdkRERZK0IyIqkqQdEVGRJO2IiIok\naUdEVCRJOyKiIknaEREVSdKOiKhIknZEREWStCMiKpKkHRFRkSTtiIiKJGlHRFQkSTsioiJzStqS\ntpV0uqRrJV0jaX9JKyStlbRO0nmStu26sRERS91ce9rvB75oey9gH+A64FjgfNt7ABcAx3XTxIiI\n6JHtme8gbQNcYXv3ScevA55me0LSKmDc9p5TPN6zxWiTJKCLeGKQryMiljZJ2Nbk43Ppae8K3CLp\nFEmXS/qIpK2AlbYnAGzfDOzQbpMjImKy5XO8z5OAV9m+TNI/UEojk7ud03ZDV69evf762NgYY2Nj\n825oRMQoGx8fZ3x8fNb7zaU8shL4D9u7NbefSknauwNjfeWRC5ua9+THpzwSETFPm10eaUogP5b0\n2ObQgcA1wNnAUc2xI4Gz2mlqRERMZ9aeNoCkfYCPAQ8AbgCOBpYBpwE7AjcCh9q+fYrHpqcdETFP\n0/W055S0Fxg4STsiYp4WMnokIiIWiSTtiIiKJGlHRFQkSTsioiJJ2hERFUnSjoioSJJ2RERFkrQj\nIiqSpB0RUZEk7YiIiiRpR0RUJEk7IqIiSdoRERVJ0o6IqEiSdkRERZK0IyIqkqQdEVGRJO2IiIok\naUdEVCRJOyKiIknaEREVSdKOiKhIknZEREWWz+VOkn4E3AHcD/za9u9IWgF8DtgZ+BFwqO07Ompn\nREQw9572/cCY7X1t/05z7FjgfNt7ABcAx3XRwIiI2GCuSVtT3PcgYE1zfQ1wcFuNioiIqc01aRv4\niqRLJb28ObbS9gSA7ZuBHbpoYEREbDCnmjbwFNs3SdoeWCtpHSWR95t8e73Vq1evvz42NsbY2Ng8\nmxkRMdrGx8cZHx+f9X6yp821Uz9AOh64C3g5pc49IWkVcKHtvaa4v+cbYyEkMcPnx0KemUG+johY\n2iRhW5OPz1oekbSVpIc017cG/gi4GjgbOKq525HAWa21NiIipjRrT1vSrsCZlO7rcuDTtt8laTvg\nNGBH4EbKkL/bp3h8etoREfM0XU973uWRzQicpB0RMU+bXR6JiIjFI0k7IqIiSdoRERVJ0o6IqEiS\ndkRERZK0IyIqkqQdEVGRJO2IiIokaUdEVCRJOyKiIknaEREVSdKOiKhIknZEREWStCMiKpKkHRFR\nkSTtiIiKJGlHRFQkSTsioiJJ2hERFUnSjoioSJJ2RERFkrQjIiqSpB0RUZE5J21JW0i6XNLZze0V\nktZKWifpPEnbdtfMiIiA+fW0jwG+13f7WOB823sAFwDHtdmwiIjY1JyStqRHAX8CfKzv8EHAmub6\nGuDgdpsWERGTzbWn/Q/AGwD3HVtpewLA9s3ADi23LSIiJlk+2x0k/SkwYftKSWMz3NXT/WD16tXr\nr4+NjTE2NtPTREQsPePj44yPj896P9nT5tpyB+kdwBHAvcCWwEOBM4H9gDHbE5JWARfa3muKx3u2\nGG2SxAyfHwt5Zgb5OiJiaZOEbU0+Pmt5xPabbe9kezfgz4ELbL8EOAc4qrnbkcBZLbY3IiKmsJBx\n2u8C/lDSOuDA5nZERHRo1vLIggOkPBIRMW+bXR6JiIjFI0k7IqIiSdoRERVJ0o6IqEiSdkRERZK0\nIyIqkqQdEVGRJO2IiIokaUdEVCRJOyKiIknaEREVSdKOiKhIknZEREWStCMiKpKkHRFRkSTtiIiK\nJGlHRFQkSTsioiJJ2hERFUnSjoioSJJ2RERFkrQjIiqSpB0RUZFZk7akB0m6WNIVkq6WdHxzfIWk\ntZLWSTpP0rbdNzciYmmT7dnvJG1l+25Jy4BvAa8FDgFutX2ipDcBK2wfO8VjPZcYbZEEdBFPDPJ1\nRMTSJgnbmnx8TuUR23c3Vx8ELKdkxYOANc3xNcDBLbQzIiJmMKekLWkLSVcANwNfsX0psNL2BIDt\nm4EdumtmRERA6TXPyvb9wL6StgHOlPR4Nq1BTFs7WL169frrY2NjjI2NzbuhERGjbHx8nPHx8Vnv\nN6ea9kYPkP4GuBt4OTBme0LSKuBC23tNcf/UtCMi5mmza9qSfqM3MkTSlsAfAtcCZwNHNXc7Ejir\ntdZGRMSU5lIeeQSwRtIWlCT/OdtflHQRcJqklwI3Aod22M6IiGAzyiPzDpDySETEvC1oyF9ERCwO\nSdoRERVJ0o6IqEiSdkRERZK0IyIqkqQdEVGRJO2IiIokaUdEVCRJOyKiIknaEREVSdKOiKhIknZE\nREUGkrQltX5ZtWqXQTQ9ImJRGcgqf4NcdS+r/EXEKMgqfxERIyBJOyKiIknaEREVSdKOiKhIknZE\nREWStCMiKpKkHRFRkSTtiIiKJGlHRFRk1qQt6VGSLpB0jaSrJb22Ob5C0lpJ6ySdJ2nb7psbEbG0\nzTqNXdIqYJXtKyU9BPg2cBBwNHCr7RMlvQlYYfvYKR6faewREfO02dPYbd9s+8rm+l3AtcCjKIl7\nTXO3NcDB7TU3IiKmMq+atqRdgCcCFwErbU9ASezADm03LiIiNrZ8rndsSiP/Chxj+65S9tjIDLWD\n1X3Xx5pLRET0jI+PMz4+Puv95rQ0q6TlwLnAl2y/vzl2LTBme6Kpe19oe68pHpuadkTEPC10adZ/\nBr7XS9iNs4GjmutHAmctqIURETGruYweeQrwdeBqShfWwJuBS4DTgB2BG4FDbd8+xePT046ImKfp\netrZuWaB8SIiupCdayIiRkCSdkRERZK0IyIqkqQdEVGRJO2IiIokaUdEVCRJOyKiIknaEREVSdKO\niKhIknZEREWStCMiKpKkHRFRkSTtiIiKJGlHRFQkSTsioiJJ2hERFUnSjoioSJJ2RERFkrQjIiqS\npB0RUZEk7YiIiiRpR0RUJEk7IqIisyZtSR+XNCHpqr5jKyStlbRO0nmStu22mRERAXPraZ8CPGvS\nsWOB823vAVwAHNd2wyIiYlOzJm3b3wRum3T4IGBNc30NcHDL7YqIiClsbk17B9sTALZvBnZor0kR\nETGd5S09j2f+8eq+62PNJSIiesbHxxkfH5/1frJnybeApJ2Bc2zv3dy+FhizPSFpFXCh7b2meaxn\nzembRUzVdkkMMl5EDMeqVbswMXFj68+7cuXO3Hzzj1p/3vmShG1NPj7X8oiaS8/ZwFHN9SOBsxbU\nuoiIeSoJ261fuvggaNOsPW1Jp1LqGQ8HJoDjgS8ApwM7AjcCh9q+fZrHp6cdEa0b9ff6dD3tOZVH\nFhg4STsiWjfq7/WFlkciImIRSNKOiKhIknZEREWStCMiKpKkHRFRkSTtiIiKJGlHRFQkSTsioiJJ\n2hERFUnSjoioSJJ2RERFkrQjIiqSpB0RUZEk7YiIiiRpR0RUJEk7IqIiSdoRERVJ0o4ZrVq1C5Ja\nv6xatcuwX1pElbLd2ALjjbr8PmOxGvX/m9luLCJiBCRpR4yolLZGU5J2ZfJGrFdXf7vp/n4TEzdS\nygftXsrzxrAsqKYt6dnAP1KS/8dtv3uK+6Sm3WYrBvz6Rv33OUjd/S5hqt/nqP/tlsLra7WmLWkL\n4APAs4DHA4dL2nPzm1in8fHxYTdhZGy33aqB9kTzt2tXfp+DsZDyyO8A37d9o+1fA58FDmqnWfXI\nf9T23HbbBF18nZ/uK33+du3K73MwFpK0Hwn8uO/2T5pjS8p73vOPqTFHkPfCoORE5AL96ld3kJM9\ndeoqySzVRJP3wmAsX8Bjfwrs1Hf7Uc2xKWxSS29FORGReKMVr5tY08frxsTEjYvk9Y3y/5VhxBu+\nzR49ImkZsA44ELgJuAQ43Pa17TUvIiL6bXZP2/Z9kl4NrGXDkL8k7IiIDnW+9khERLQnJyIjIiqS\npB1DJWlLSXsMux0RtegkaUvaXdKDmutjkl4r6WFdxBoGSaskPU/ScyWtGnZ7aiXpucCVwJeb20+U\ndPZwWxVz1XuPz3Ys2tVJTVvSlcB+wC7AF4GzgMfb/pMOYv0t8Dbb9za3twHeb/votmM1z/9y4K3A\nBZTxRk8DTrD9z13E64v7SGBn+k4e2/56yzHOYYbFHGw/r+V43waeAYzb3rc5drXt32ozzhRxHwt8\nCFhp+wmS9gaeZ/vvOoj1eeDjwJds39/28zcxrmbqv5sA2967o7iX237SbMc6iPt7lNzS/174ly5j\nLiYLGac9k/tt3yvpz4CTbZ8s6YqOYi0HLpZ0NLCSsh7KyR3FAngDsK/tWwEkPRz4d6CzpC3p3cBh\nwPeA+5rDBlpN2sB7mn+fD6wCPtXcPhyYaDkWwK9t3zFpTOwgzox/lPJ3/H8Atq+SdCrQetIGPggc\nDZwk6XTgFNvrWo7xnJafb0bNt8tHAltK2pcNg6W3AbbqOPYngd0p39D63wtJ2gv0a0mHA0cCz22O\nPaCLQLaPk3Q+cDFwG/AHtq/vIlbjVuDOvtt3Nse6dDCwh+3/7TKI7a8BSHqv7f36fnSOpMs6CHmN\npBcByyQ9Bngt5QOwa1vZvmTSh8W9XQSyfT5wvqRtKR9+50v6MeWD41PNuj0LjTHoKYPPAo6iTKh7\nX9/xO4E3dxx7P+BxXsLD3rpK2kcDrwTebvuHknYFPtlFIEl/AJwEnAD8FnCypJfZ/lkX8YDrKT37\nsyif8AcBV0l6PYDt98304M10A+VDr9Ok3WdrSbvZvgGg+ftt3UGc1wBvobyuzwDnAX/bQZzJbpG0\nO02vXtILKBPEOtF8GzsCeAlwBfBp4KmUTs1YC89/JzOXR7ZZaIx+ttcAayQdYvuMNp97Dr5L+RbY\n2d9rsRvEHpErgB1tX9XR818CHGX7e83t5wPvsN3JMrGSjp/p57bf1mKskylvxkcC+wBfpS9x235t\nW7EmxX028BHKh4UotfS/tH1eF/EGTdJulNf3e5RvZz8EjrD9ow5inQnsQem0fML2TX0/u2zSN5rq\nSPpTytLMD+4ds31Ch/EuBJ5ImYHd/15o9XzLYtbVichx4HmUnvy3gZ8D37L9+g5iLbN936RjD+/V\nnLvUfCDd3tVXNUlHzvTzpsfTiWYUQO+D77o2SzODPuE5Qzu2Brawfeesd978GE+3fWFXz9/E2Mb2\nLyVtN9XPbf+io7gfptSwnw58DHgBcIntl3URr4n5tKmO90p7S0FXSfsK2/s2Iy12tH28pKu6OIst\naSXwDuCRtp8t6XHA79r+eMtx3gqcZvu6JqF9ifKJfy/woqZ22YkmufxP78OpWfflQbbv7ijeVsDr\ngZ1t/5+m3ryH7XNbev4p33g9Xb8BJd0H/D1wXO8Dt+1RD803vmnZ/nyLsc61/RxJP6R8GPYX6217\nt7ZiTYp7le29+/59CGWUzO93Ea8v7krgyc3NS2z/vMt4i01Xk2uWS3oEcCjQyht9Bp+g1EIf0dz+\nT+CvOohzGGWBLCi1yC2A7SlD/t7RQbx+XwW27Lu9JdDZhwRwCnAP8LvN7Z/S4sgK219rEvMTe9f7\nj7UVZwbXUP5+a/t6p20v6/bcGS6tjvaw/Zzm311t79b827t0krAb/938e7ek3wR+zYb3YSckHUop\njbyQkl8ubs5JLBldnYg8gZJIv2n70qaG+P2OYv2G7dMkHQfQDDW8b7YHbYZ7+sogzwI+0/R8r5XU\n1e+x58G27+rdsH1X0xvuyu62D2tGAGH7bnWzVuWRwPsnHTtqimNtu9f2GyUdBnxD0l/Q8lDDruYJ\nTEXSq21/oLn+eNvXDCj0uc2kub8HLqf8Dj/Wccy3AE/u9a4lbU/pwPxrx3EXjU6Sje3TgdP7bt8A\nHNJFLOBXzdn53tfcA4A7Oojzv5KeQBmv/HTgr/t+1unYVMprfJLtywEk/TYbejlduEfSlmz4ne5O\niyNXmg+DFwG7TpoB+VCgk/rr5CYA2P6cpGuAU9l4bfiFB5COsP2p3qiiyVoeZfRSyvwEKCc8O53c\n0mO7N9LnDEnnUjoXXbz3+m0xqRxyK0tsOY5OkrakBwMvY9Ozyi/tINzrgbOB3SV9i1Ky6OLr0jGU\nT/PtgX+w/UMASX9CGcbVpWOA0yX9jJJwVlHKNV05njK1fEdJnwaeQukBt+XfKUO2fgN4b9/xO4FO\nRhlN8vLeFdvflfT7tL+/aW+I5ENbft7ZdL56v6QjKOfD1g/jtf2/kg6VdJ/tUzsM/2VJ51GGiEJ5\nH3yxw3iLTlcnIk8HrqP0pk4AXgxca/uYFmM8Gfix7Zub8sRfUnrz3wPe2tUZ80FT2fX+AOBSytAx\ngHVtTMqYJe7Dm7gCLrJ9S5fxBkHSM2xfMN1JwjZPDg6SpBuA/0vpcZ5Ime25XtuvS9LFwIH9Jbvm\n+NbA123/dpvxpoj/fMo4d4Bv2D6zy3iLTdejR3pnlR9A+eUe0GKMy4Fn2v5FM8Hms5TJGk8E9rLd\nycmJJpkdT/lPY+CblLVHOhti2Pt9dvX8U8Q7wfZb+25vAXzS9otbev5v2n7qFJNCOpkM0hf3bc1I\nplOm+LG7+CbYTEx6DZuuldHasMZpXk9fqHZf10wjbboaJdb3/OtHUqmsDrkHZcRKp52YxaSzaezN\nv7c3deCbgR1ajrGsrzd9GPARl9lZZ6gsWNWVz1LW/OjV6F8MfA54ZocxvyrpEODzXY0Jn2RHScfZ\nfmczvPE02i0BbQ1ge6ClA9vHN/8O7CQh8AXKglHnAJ0sGDXg1wNlzZGtbf+q/6CkhwIP7Dj214Hf\nb+ZIfBm4jPL+b6VDUQXbrV8oNcMVlOFwN1Am17yy5RjfBZY316+jrDmy/mddvK7pnhu4uqt4zfPf\nSXnD3wP8srn9yw7jiXJy7jjKdnKva/n5L+/y9zVD3OdSxp73br8V+A7lnMiuHcW8eICvbyUbVhQE\neBzwsg7i/DVlnkL/73IX4N+AN3T8Gi9v/n0N8Mbm+pXD+P80rEtXo0d6w36+BnQ1TvQzwNck3UIZ\nSfENAEmPppvRIz1rJf05pfcJ5aRnp9O7PaAeqaT+r7zvp6yC9y3K73n96JUW7DDdqArobP0WgLdT\n6vRIeg5lPZDDgX2BD1OGcrbt/SpLH6xl42nXbf0u+32CMsb+Lc3t/6R8C2x1opnt90i6C/h6M6FG\nlI7Eu2x/qM1YU5Ck36X0rHszL5d1HHNRabWmPdMbEdp/MzbD+x4BrHXzVU1lreSHtP2m6Ku/ivL1\nvjcWfBlwlzuqw/bFXwE8ho1H47S9nvZM061t+xktxbmJsp71lCMd3OL6LZPifsf2Ps31f6ac0H13\nc7uTdaAlvZOyUNQP2FAeae13OSnWpbaf3H8ORNKVtjubsNSURHCHSwFMivcHlJ7+t2y/u5kD8lfu\naB2exajtnvaga5QXTXHsPzuKNeihW+upLAdwDGUpzCspvcX/oGwg0BrbT29OOr7Q9ufafO5JbnKH\niwrNQE3P8G7gQMpa1z0PnvohC/ZCYDfb93T0/P0GNWeB5vl7S0j8JvDH6mgJib54yyibVaw/iesy\nB2TJJGxoOWl31UNaDCTt6bLuyJS9sY6+7vYcQ1lr4aImse5JR1Pnbd8v6Q2Ur9Vd6Xws8TT+kfKh\n90vKENTLAFQW8u9qqc/vAg+jnNfp2qDmLPR8ggGUY3pcRow8dfZ7jrauhvytAY6xfXtzewXwXncz\nuWYgJH3E9ismlRDW//K6+LrbF7v3tfdKYH+XiQzX2H58R/HeBdxCeQOuHyHglsa+S9qurefajNiP\npIxk+o6b7b9U1sl5gO3/am63NhVcZcXLvSnj7DtfSrSZs7AH5YOx0/H8QyrHfIiyVPHpbPx/s8ox\n9pujqyF/e/cSNoDt25reTM0+JmmV7afD+mVTDwF+BKzuOPZPVNZ4+ALwFUm3AV3uVtKbbfmqvmOm\npZPKw0rYTeyfUhbA6j82uZfd5lTwGddfb9MUk4YeK+kOyuimLnr6Ay3HNB5Mmbre30kysGSSdlc9\n7e8AY7Zva25vB3zNHW/Y2qVhTeaZoh1PA7YFvjygOumSM+jJTG2R9G+UlRl73wbHKOvZ70qZANbq\n7lFNqfBk4AmUMtD2wAvc0YYnUXTV034vcJGk3rC4F1KGW9Vs4JN5VNZweSXwaOBq4OMe0GLvzaSo\nx7HxaJWlsnlqaz2Zpvd5MrAXZeLJMuBXHY02Wk7pQEw0sVdSNrzdnzIppdWkbfvyphMxkHIMrB8d\n9iFgpe0nSNqbcnKyi02ZF6Wuxmn/i8pGsL2vMM93sx1YxZZJWm77XsrIg1f0/ayrD781lNml3wD+\nmJJEW1u/ZTrNuOKxJt4Xm9jfZAnteN2iDwB/TqnB7gf8BfDYjmLt2EvYjZ83x34hqfVkOoRyDJQN\nkd9AmUOA7asknUqL670vdq0mmyl6hh9uktwoGMZknsf1SkqSPk5Z/H0QXkDZk/IK20c3PbZPDSj2\nYtBq2cn29dqwLd4pkq6gzDZt27jKEqm9ZZEPaY5tDdw+/cM228uYphyjsn5NF5t5b2X7Em28vPuo\n5Jg5abuHOLlnuBfd7CIzcLbfLumrbJjM0/sKvQWltt2F9b0jl80dOgqzif9uhv7dK2kbmh7boIJ3\nTdLn2TDde5P1QNziwmaUXV0eCFwp6UTK0MKu1n9+FdC/At5llDLCryhrwLdtoOWYxi0q67v3Tn6+\ngCW2M3vbSXtYPcOBGORknsY+kn7ZXBdloZ5f0vFqeMBlzWiVj1J6TndRJvOMig8CRwMnqSwjfIrt\ndbM8ZnO9hJKkXw28jvLh18mGILatskzrAZTzSD8EzugiVmOg5ZjGq4CPAHtK+inlNS6dxaJofxr7\nRlOBu5oaHIMjaRdgm1EcESBpW8raI28Bfkz5kPpUGyfTJO3UG/fdtebk3OHNpTe+/q9t79xx3A9S\ndvzpL8f8hFJzPrc3PLaj2FtTdrEZyPT5xaTtpH0fGwa8i7IB7d103zOMlmnDQvOm7PU5UgvNN+OL\nj6D0hH8GfJryen/L9lgLz7++wyLpDNtdbbeHpPspJcmX2b6+OXaDu93UF5V6XX855jZKOeZV0z9q\ns2PtT+lh7045X/ZS29e2HacGrdbWbC+zvU1zeajt5X3Xk7Ar0fSgXkl5c3wX+EtJ/zTcVrVH0pmU\nJLcV8Fzbz7P9OduvAR7SVpi+650mT0rivAm4UNJHJR04KX4nmvM6N1BOBP4ZpW7eVSL9J8pCUQ8H\n3kdZkmBJ6mRyTdRN0nWUE0y9kz1bANfY3mu4LWuHpKfbnmlFwzZi9Pe0B1ImbEoGB1HKJM+gnBQ8\n0/baluMMvByT0usGXY0vjrpdT6lV9qbK79gcq1r/uOIpxhi3vX5F7yRy/wlk6LBU2IwSORU4tVnv\n54XAmyhrebfpOso3lef0lWNe13KMyR426W+20e2ltPZIetqxnqRzKDXsbSmrCl7S3N4fuKSNWu8w\nacB7KY4qSQdTJgw9hbLl12eBj9netcOY+ds1krRjvWZK8rQGNYU+6jCocsw823Sk7TXDij8ISdox\nrWZiTf8O4kNbna8Nko6w/SlNs8OSu9vmbOT1lWMOs33gENsx8rXu1LRjE5JeAZwA/A9liyzR4tKs\nQ7R18+/QdiEaVc2Knh9pLsM0rA02BiY97diEpO9Tto26ZdhtiZiP9LRjqfoBZVLUSJK0K2W9mF3Y\nuPzTyW4yMVAj39NO0o6pHAf8u6SL2XiLrFHZQPULlAWjzmHDDumxyDXzBV5g+7QZ7vatQbVnWFIe\niU1IuoSyfvbV9CW1UTkrL+li2/sPux0xf5Ius73fsNsxTEnasYlat9uaK0kvAh5DmXTS/03i8qE1\nKuZEHW86XYMk7diEpHdQNiw+h42T2ki8MSS9k7JQ1A/Y8E3Ctp8x/aNiMZD0wykOu+vFsRaTJO3Y\nxKi/MSRdT1n7PRsjR3VyIjI20eV05EXiu8DDKIv2R0UkbQW8HtjJ9iskPQbYw/a5Q27awHS17VFU\nSNIb+66/cNLP3jH4FnXmYcB1ks6TdHbvMuxGxZycQtnD8/ea2z9lCW3qCymPRJ+ZlhMdpUkL062x\nkrVVFr/e6JH+k+WSvmN7n2G3bVBSHol+mub6VLerleRctXskbcmGjX13p+9k+VKQ8kj08zTXp7pd\nLUkHSLpU0l2S7pF0X99617G4raYsB7ujpE8DXwXeOOMjRkzKI7Fe3x6f/ft70tx+sO0HDKttbZJ0\nGWU96NOB/YC/AB5r+7ihNizmpNnf8wDK/8uLltoaOUnaseT01UWvsr13c2ykJxTVTtIOwJuBR1Nm\n6r7T9pL8dpTySCxFd0t6IHClpBObrbLyXljc/oXyLfBkyubLJw23OcOTnnYsOZJ2BiaABwKvo2yv\n9sHefodvBm15AAACTElEQVSx+EweITJKo5nmK6NHYsmQtJPt/7Ld27D4f4C3DbNNMXfN7ji9UUzL\n+m+PyhILc5GediwZk8ahn2H7kGG3KeZG0o/YsIvSZCOzxMJcpKcdS0n/G37JvMlHge1d5nI/SY+3\nfU3HzRmqnHyJpWSmcegxGj457AZ0LT3tWEr2aSbRCNiyb0KNKF+xtxle06IlIzNzdzpJ2rFk2F42\n7DZE50b+G1TKIxERFUnSjohRMvIbW2TIX0RUQ5KAFwO72T5B0k7AKtuXDLlpA5OkHRHVkPQhynjt\nZ9jeq5lgs9b2k4fctIHJiciIqMn+tp8k6QoA27c168gsGalpR0RNfi1pGRs2Qdie0vNeMpK0I6Im\nJwFnAjtIejvwTWCU9i+dVWraEVEVSXsCB1Im0nzV9rVDbtJAJWlHRBWassg1tvccdluGKeWRiKiC\n7fuAdc0wvyUro0cioiYrgGskXULZyQYA288bXpMGK0k7ImryN8NuwLClph0RUZHUtCOiGpIOkHSp\npLsk3SPpvr4ldpeEJO2IqMkHgMOB7wNbAi8H/mmoLRqwJO2IqIrt64Fltu+zfQrw7GG3aZByIjIi\nanJ3s9bIlZJOBG5iiXU+l9SLjYjqvYSSt15NGfK3I3DIUFs0YBk9EhGLnqSdbP/XsNuxGKSnHRE1\n+ELviqQzhtmQYUvSjoga9O+yvtvQWrEIJGlHRA08zfUlJzXtiFj0JN1HOfEoyvjsu3s/Amx7m2G1\nbdCStCMiKpLySERERZK0IyIqkqQdEVGRJO2IiIr8f/B26mzXXXoFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2045ed950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821548821549\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "predictors = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Family_Size', 'Family', 'AgeFill', 'AgeCat', 'Fare_Per_Person']\n",
    "\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic['Survived'])\n",
    "\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "predictors = ['Pclass', 'Sex', 'Fare', 'Title']\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some ensembling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:32: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"AgeFill\", \"Fare\", \"Embarked\", \"Family_Size\", \"Title\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"Family_Size\", \"Title\", \"AgeFill\", \"Embarked\", \"AgeCat\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to match these same changes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId        418\n",
       "Pclass             418\n",
       "Name               418\n",
       "Sex                418\n",
       "Age                332\n",
       "SibSp              418\n",
       "Parch              418\n",
       "Ticket             418\n",
       "Fare               418\n",
       "Cabin               91\n",
       "Embarked           418\n",
       "Title              418\n",
       "Family_Size        418\n",
       "Family             418\n",
       "AgeFill            418\n",
       "AgeCat             418\n",
       "Fare_Per_Person    418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test = pandas.read_csv('test.csv')\n",
    "\n",
    "titanic_test = clean_data(titanic_test)\n",
    "\n",
    "# Making sure there aren't any NaN values\n",
    "titanic_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it's time to predict and make a submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"AgeFill\", \"Fare\", \"Embarked\", \"Family_Size\", \"Title\", \"AgeCat\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"Family_Size\", \"Title\", \"AgeFill\", \"Embarked\", \"AgeCat\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggleForestHalfNew2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After several submissions, and lots of experimenting with different types of feature engineering, parameter tuning on the random forests, and some other assorted tricks, I topped out at ~79%. Even after I did a good amount of research into what extra feature engineering would help me out, my score ended up decreasing (not significantly, I simply predicted one fewer person correctly).\n",
    "\n",
    "Upon this, I did more research into random forests, and learned pretty quickly that they are very prone to overfitting. Here's an excerpt taken from a list of disadvantages scikit-learn mentions about decision trees and random forests -\n",
    "\n",
    "\"Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some notes/thoughts I've come across on Random Trees, Gradient Boosted, etc.\n",
    "\n",
    "Algorithmically, what's the difference?\n",
    "\n",
    "Random Forests are trained with random sample of data (even more randomized cases available like feature randomization) and it trusts randomization to have better generalization performance on out of train set.\n",
    "On the other spectrum, Gradient Boosted Trees algorithm additionally tries to find optimal linear combination of trees (assume final model is the weighted sum of predictions of individual trees) in relation to given train data. This extra tuning might be deemed as the difference. Note that, there are many variations of those algorithms as well.\n",
    "\n",
    "Practicallly, what's the difference?\n",
    "\n",
    "Due to the extra necessary tuning stage, Gradient Boosted Trees are more susceptible to jiggling data. This final stage makes GBT more likely to overfit therefore if the test cases are inclined to be so verbose compared to train cases this algorithm starts lacking. On the contrary, Random Forests are better to strain on overfitting although it is lacking on the other way around.\n",
    "\n",
    "Two takeaways, then:\n",
    "\n",
    "1. RF are much easier to tune than GBM\n",
    "2. RF are harder to overfit than GBM\n",
    "\n",
    "Given this, perhaps I should have ended up using the RF in my combination - I don't feel like I have enough experience to make an informed decision on tuning parameters, so perhaps I'll just use a Random Forest, which c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2), [\"Pclass\", \"Sex\", \"AgeFill\", \"Fare\", \"Embarked\", \"Family_Size\", \"Title\", \"AgeCat\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"Family_Size\", \"Title\", \"AgeFill\", \"Embarked\", \"AgeCat\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggleRFLogistic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After losing another percentage of accuracy, bringing me down to 78%, I've decided to call it a day. Perhaps with enough parameter tuning, I could get it to increase, but I think this is exposing limitations of my feature engineering. If I were to continue to work on this, feature engineering would be one of my first areas of attack - only then would I start exploring different models, or perhaps tuning the ones I've been dealing with thus far - logistic regression, random forests, and gradient boosted trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
